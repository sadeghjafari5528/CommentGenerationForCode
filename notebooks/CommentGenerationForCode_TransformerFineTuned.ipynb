{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CodeTrans: code_x_glue_cc_code_to_code_trans fine-tune example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install the necessary libraries"
      ],
      "metadata": {
        "id": "3G-Lwu98bSwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTWVPZeNe3Ve",
        "outputId": "328f5b9e-15ff-4953-de2f-bd74758dd5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the necessary libraries"
      ],
      "metadata": {
        "id": "r_8795V_bba_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, SummarizationPipeline\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, Text2TextGenerationPipeline\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "FJ1w2T3Ce7X0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model and its tokenizer libraries"
      ],
      "metadata": {
        "id": "Ukkc91nsbd-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"SEBIS/code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "iLCuMJkphBur"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the evaluation metric"
      ],
      "metadata": {
        "id": "cg6DEpoabkgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_metric = load_metric(\"bleu\")"
      ],
      "metadata": {
        "id": "BCV_HB2ye_0c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "s4GaPzBgboOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data.json\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "codes = []\n",
        "comments = []\n",
        "for sample in data:\n",
        "  codes.append(sample[\"method_text\"])\n",
        "  comments.append(sample[\"comment_text\"])"
      ],
      "metadata": {
        "id": "tFa83HRN9AuS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the dataset"
      ],
      "metadata": {
        "id": "7NloGQ1Kbxv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 512\n",
        "source_input = \"code\"\n",
        "target_output = \"comment\"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[source_input]\n",
        "    targets = examples[target_output]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "GYu-lCC2gcR7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenized_datasets(codes, comments, train, val, test):\n",
        "  no_data = len(codes)\n",
        "\n",
        "  train_data = []\n",
        "  for i in range(0, int(no_data*train)):\n",
        "    train_data.append(preprocess_function({\"code\":codes[i], \"comment\":comments[i]}))\n",
        "  #train_data = preprocess_function(train_data)\n",
        "\n",
        "  val_data = []\n",
        "  for i in range(int(no_data*train), int(no_data*(train + val))):\n",
        "    val_data.append(preprocess_function({\"code\":codes[i], \"comment\":comments[i]}))\n",
        "  #val_data = preprocess_function(val_data)\n",
        "\n",
        "  test_data = []\n",
        "  for i in range(int(no_data*(train + val)), int(no_data*(train + val + test))):\n",
        "    test_data.append(preprocess_function({\"code\":codes[i], \"comment\":comments[i]}))\n",
        "  #test_data = preprocess_function(test_data)\n",
        "  return {\"train\":train_data, \"validation\":val_data, \"test\":test_data}"
      ],
      "metadata": {
        "id": "RQwlfZyZ9hG7"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = get_tokenized_datasets(codes, comments, 0.8, 0.1, 0.1)"
      ],
      "metadata": {
        "id": "q99VKcbZ_FhB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the training arguments"
      ],
      "metadata": {
        "id": "hJVuqox0b4fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_input}-to-{target_output}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        "    fp16_opt_level=\"02\",\n",
        "    push_to_hub=False,\n",
        "    gradient_accumulation_steps=32,\n",
        "    seed=42,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "PSg_T-1WiVcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061e2c98-70ce-4a85-bb3c-de00c9c425e9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the data collator for the inputs/outputs batching"
      ],
      "metadata": {
        "id": "ciU55DCqcM24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "wPBmPtyIjScd"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the evaluation metric function"
      ],
      "metadata": {
        "id": "V98YX7J8cU-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip().split() for pred in preds]\n",
        "    labels = [[label.strip().split()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"bleu\"]*100}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "OJgV1qW-jWXA"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the trainer based on the above declarations and functions"
      ],
      "metadata": {
        "id": "QxzIu6SYccvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "p2nq9zPPjiyD"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start training"
      ],
      "metadata": {
        "id": "sdtBsKV7cnOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IqHaNwIxjlNB",
        "outputId": "3677a10e-f4c7-4f4a-e25b-18d5d0ee0b4e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 4292\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 32\n",
            "  Total optimization steps = 160\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 46:54, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.657360</td>\n",
              "      <td>1.276700</td>\n",
              "      <td>16.330200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.504279</td>\n",
              "      <td>5.762000</td>\n",
              "      <td>17.949600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.038161</td>\n",
              "      <td>6.849900</td>\n",
              "      <td>17.759300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.816769</td>\n",
              "      <td>7.987300</td>\n",
              "      <td>17.925400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.716589</td>\n",
              "      <td>7.445400</td>\n",
              "      <td>16.876900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.639648</td>\n",
              "      <td>7.550300</td>\n",
              "      <td>16.589600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.605978</td>\n",
              "      <td>7.756700</td>\n",
              "      <td>16.804100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.577969</td>\n",
              "      <td>7.808400</td>\n",
              "      <td>16.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.556531</td>\n",
              "      <td>7.976700</td>\n",
              "      <td>16.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.550891</td>\n",
              "      <td>7.876300</td>\n",
              "      <td>16.710800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-12] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-108] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-120] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-16] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-32] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-48] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-80] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-96] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-144\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-144/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-144/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-144/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-144/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-144/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-112] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 536\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-160\n",
            "Configuration saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-160/config.json\n",
            "Model weights saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-160/pytorch_model.bin\n",
            "tokenizer config file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-160/tokenizer_config.json\n",
            "Special tokens file saved in code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-160/special_tokens_map.json\n",
            "Copy vocab file to code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-160/spiece.model\n",
            "Deleting older checkpoint [code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-128] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from code_trans_t5_small_code_comment_generation_java_transfer_learning_finetune-finetuned-code-to-comment/checkpoint-64 (score: 7.9873).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=4.013628387451172, metrics={'train_runtime': 2823.3089, 'train_samples_per_second': 15.202, 'train_steps_per_second': 0.057, 'total_flos': 3886639061532672.0, 'train_loss': 4.013628387451172, 'epoch': 9.95})"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create inference pipeline"
      ],
      "metadata": {
        "id": "jBiA0ldAc5hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_pipeline = SummarizationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0\n",
        ")"
      ],
      "metadata": {
        "id": "M6NnnsE0Cj_E"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = SummarizationPipeline(\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0\n",
        ")"
      ],
      "metadata": {
        "id": "E7LsjKCKl_DY"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make prediction for a single example on the test tdataset"
      ],
      "metadata": {
        "id": "fnqKCjQdc-4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install javalang\n",
        "import javalang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv0CyGN63sT7",
        "outputId": "be45ff2a-c84e-4a2d-9c03-fbc8e0eaf455"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: javalang in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from javalang) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_java_code(code):\n",
        "    tokenList = []\n",
        "    tokens = list(javalang.tokenizer.tokenize(code))\n",
        "    for token in tokens:\n",
        "        tokenList.append(token.value)\n",
        "    \n",
        "    return ' '.join(tokenList)"
      ],
      "metadata": {
        "id": "bjgn_YS63v95"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = codes[5010]"
      ],
      "metadata": {
        "id": "5B9pmqPa298R"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_code = tokenize_java_code(code)\n",
        "print(\"Output after tokenization: \" + tokenized_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKqE4bPR226J",
        "outputId": "7e8afae9-2b03-491b-ca7c-56c356084322"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output after tokenization: void debugPrintln ( String msg ) { if ( DEBUG ) { System . err . println ( \"XERCES: \" + msg ) ; } }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(comments[5010])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BZ19Cb8UUiI",
        "outputId": "f66d83e5-f2f1-454d-8d55-2ba2f8017dfd"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Prints a message to standard error if debugging is enabled. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(original_pipeline([tokenized_code])) # original model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXA47F0tCKPq",
        "outputId": "e637c5ea-3d20-408f-a40a-e900a33da2f8"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'Prints a message to System.err .'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipeline([tokenized_code])) # fine tuned model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZWs3IIz3CFQ",
        "outputId": "64965d33-a8ea-4c85-af5e-cda91e7f8026"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'Prints a message to System.err .'}]\n"
          ]
        }
      ]
    }
  ]
}